{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "651155d0"
      },
      "source": [
        "# Task\n",
        "Implement the gradient descent algorithm to find the local minimum of the function y=(x+3)² starting from the point x=2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33c2205"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function and its derivative as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0de7778"
      },
      "source": [
        "def function_y(x):\n",
        "\n",
        "  return (x + 3)**2\n",
        "\n",
        "def derivative_y(x):\n",
        "\n",
        "  return 2 * (x + 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8639331d"
      },
      "source": [
        "## Implement gradient descent\n",
        "\n",
        "### Subtask:\n",
        "Write a Python function to implement the gradient descent algorithm. This function will take the starting point, learning rate, and number of iterations as inputs and return the optimized value of x.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c10085bc"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the gradient descent algorithm according to the instructions, including initializing x, looping through iterations, calculating the step size, and updating x.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6d8f629"
      },
      "source": [
        "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
        "  \"\"\"Implements the gradient descent algorithm to find the local minimum.\n",
        "\n",
        "  Args:\n",
        "    starting_point: The initial value of x.\n",
        "    learning_rate: The step size multiplier.\n",
        "    num_iterations: The number of iterations to perform.\n",
        "\n",
        "  Returns:\n",
        "    The optimized value of x.\n",
        "  \"\"\"\n",
        "  x = starting_point\n",
        "  for _ in range(num_iterations):\n",
        "    step_size = learning_rate * derivative_y(x)\n",
        "    x -= step_size\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "321b10a8"
      },
      "source": [
        "## Run gradient descent\n",
        "\n",
        "### Subtask:\n",
        "Call the gradient descent function with the initial point x=2, a chosen learning rate, and a number of iterations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4cbc78"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the gradient_descent function with the specified parameters, store the result, and print it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "643c6ecc"
      },
      "source": [
        "optimized_x = gradient_descent(starting_point=2, learning_rate=0.1, num_iterations=100)\n",
        "print(optimized_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fbf5ad"
      },
      "source": [
        "## Visualize the results\n",
        "\n",
        "### Subtask:\n",
        "Plot the function and the steps taken by the gradient descent algorithm to show how it converges to the minimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94e1d75a"
      },
      "source": [
        "**Reasoning**:\n",
        "Plot the function and the steps taken by the gradient descent algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83d613a3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
        "\n",
        "  x = starting_point\n",
        "  x_steps = [x]\n",
        "  for _ in range(num_iterations):\n",
        "    step_size = learning_rate * derivative_y(x)\n",
        "    x -= step_size\n",
        "    x_steps.append(x)\n",
        "  return x, x_steps\n",
        "\n",
        "\n",
        "x_values = np.linspace(-7, 1, 400)\n",
        "\n",
        "# Calculate the corresponding y values for the function\n",
        "y_values = function_y(x_values)\n",
        "\n",
        "# Generate the plot of the function y=(x+3)²\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, y_values, label='y=(x+3)²')\n",
        "\n",
        "# Run gradient descent and get the steps\n",
        "optimized_x, x_steps = gradient_descent(starting_point=2, learning_rate=0.1, num_iterations=10)\n",
        "\n",
        "# Plot the steps taken by the gradient descent algorithm\n",
        "y_steps = function_y(np.array(x_steps))\n",
        "plt.plot(x_steps, y_steps, 'o-', color='red', label='Gradient Descent Steps')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Gradient Descent on y=(x+3)²\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}